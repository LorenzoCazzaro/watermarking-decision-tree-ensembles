{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "F0xOMecfjsf4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from joblib import dump, load\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_forest(ensemble, filename):\n",
    "    dump(ensemble, filename  + \".joblib\")\n",
    "    \n",
    "def generate_random_bitmask(n_bits, perc_1):\n",
    "    attacker_bitmask = []\n",
    "    for i in range(int(perc_1*n_bits)):\n",
    "        attacker_bitmask.append(1)\n",
    "    for i in range(n_trees - int(perc_1*n_bits)):\n",
    "        attacker_bitmask.append(0)\n",
    "    random.shuffle(attacker_bitmask)\n",
    "    return attacker_bitmask\n",
    "    \n",
    "def extract_paths_from_bitmask(paths, attacker_bitmask, true_label):\n",
    "    paths_to_convert = []\n",
    "    for i, path_x_tree_list in enumerate(paths):\n",
    "        path_to_covert_x_tree = []\n",
    "        for path_in_tree in path_x_tree_list:\n",
    "            if path_in_tree[-1][1] == true_label and attacker_bitmask[i] == 0:\n",
    "                path_to_covert_x_tree.append(path_in_tree)\n",
    "            if path_in_tree[-1][1] != true_label and attacker_bitmask[i] == 1:\n",
    "                path_to_covert_x_tree.append(path_in_tree)\n",
    "        paths_to_convert.append(path_to_covert_x_tree)\n",
    "    return paths_to_convert\n",
    "\n",
    "def create_constraint_problem(paths_to_convert):\n",
    "    trees_paths_to_conj = []\n",
    "    list_elem = []\n",
    "    for paths_x_tree in paths_to_convert:\n",
    "        tree_paths_to_disj = []\n",
    "        for path_in_tree in paths_x_tree:\n",
    "            path = []\n",
    "            for node in path_in_tree[:-1]:\n",
    "                list_elem.append(node[0])\n",
    "                if(node[1] == '<='):\n",
    "                    constr = elem[node[0]] <= node[2] - 0.0001 \n",
    "                else:\n",
    "                    constr = elem[node[0]] > node[2] + 0.0001\n",
    "                path.append(constr)\n",
    "            tree_paths_to_disj.append(And(*path))\n",
    "        trees_paths_to_conj.append(Or(*tree_paths_to_disj))\n",
    "    constr = And(*trees_paths_to_conj)\n",
    "    return constr, list_elem\n",
    "\n",
    "def generate_instances(constraints, n_instances, list_elem, ensemble):\n",
    "    list_status_problems = []\n",
    "    list_values = []\n",
    "    tot_time = 0\n",
    "    for i in range(n_instances):\n",
    "        s = Solver()\n",
    "        s.add(constraints)\n",
    "        res = s.check()\n",
    "        list_status_problems.append(res)\n",
    "        if res == z3.sat:\n",
    "            #print(\"Sat {}\".format(i), end = \"  -  \")\n",
    "            for k, v in s.statistics():\n",
    "                if(k == 'time'):\n",
    "                    #print(\"%s : %s\" % (k, v))\n",
    "                    tot_time += v\n",
    "            m = s.model()\n",
    "            list_internal = []\n",
    "            sol_values_constr = []\n",
    "            list_elem = list(set(list_elem))\n",
    "            found_instance_from_z3 = []\n",
    "            for i in range(len(list_elem)):\n",
    "                sol_values_constr.append(elem[i] == m[elem[i]])\n",
    "            #print(Not(And(*sol_values_constr)))\n",
    "            constraints = And(constraints, Not(And(*sol_values_constr))) \n",
    "            for i in range(ensemble.n_features_in_):\n",
    "                if m[elem[i]] == None:\n",
    "                    list_internal.append(float(0))\n",
    "                else:\n",
    "                    frac = m[elem[i]].as_fraction()\n",
    "                    list_internal.append(float(frac.numerator) / float(frac.denominator))\n",
    "            list_values.append(list_internal)\n",
    "        else:\n",
    "            #print(res, end = \", \")\n",
    "            break\n",
    "        \n",
    "    return list_status_problems, list_values, tot_time, res\n",
    "\n",
    "def check_predictions_with_watermark(instances, labels, ensemble, watermark):\n",
    "    for instance, label in zip(instances, labels):\n",
    "        for i, tree in enumerate(ensemble):\n",
    "            predicted_label = int(tree.predict(np.array(instance).reshape((1, len(instance))))[0])\n",
    "            if (watermark[i] == 0 and predicted_label != label) or (watermark[i] == 1 and predicted_label == label):\n",
    "                print(\"Broken prediction on tree {} with class {} from tree, true label {} and watermark bit {}\".format(i, predicted_label, label, watermark[i]))\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def generate_extra_constraint(constr, eps, elem, test_instance):\n",
    "    constr_add = []\n",
    "    for i in range(len(elem)):\n",
    "            constr_add.append(elem[i] >= test_instance[i] - eps)\n",
    "            constr_add.append(elem[i] <= test_instance[i] + eps)\n",
    "            constr_add.append(elem[i] >= 0)\n",
    "            constr_add.append(elem[i] <= 1)\n",
    "    constr = And(constr, And(*constr_add))\n",
    "    return constr\n",
    "\n",
    "def generate_range_constraint(constr, eps, elem, test_instance):\n",
    "    constr_add = []\n",
    "    for i in range(len(elem)):\n",
    "            constr_add.append(elem[i] >= 0)\n",
    "            constr_add.append(elem[i] <= 1)\n",
    "    constr = And(constr, And(*constr_add))\n",
    "    return constr\n",
    "\n",
    "def generate_random_instance():\n",
    "    synt_instance = []\n",
    "    for f in range(len(elem)):\n",
    "        synt_instance.append(random.random()) \n",
    "    #random label\n",
    "    synt_instance_label = random.randint(0,1)\n",
    "    return synt_instance, synt_instance_label\n",
    "\n",
    "def generate_bitmask(instance, instance_label): \n",
    "    attacker_bitmask = []\n",
    "    for t in ensemble_rit:\n",
    "        for pred in t.predict([instance]):\n",
    "            if pred == instance_label:\n",
    "                attacker_bitmask.append(0)\n",
    "            else:\n",
    "                attacker_bitmask.append(1)\n",
    "    return attacker_bitmask\n",
    "\n",
    "def compute_accuracies_watermarked_model(ensemble, X, y):\n",
    "    arr_ = []\n",
    "    for i, tree in enumerate(ensemble):\n",
    "        arr_.append(tree.predict(X))\n",
    "        \n",
    "    ones_ = 0\n",
    "    zeros_ = 0\n",
    "    pred_ = []\n",
    "    for j in range(len(arr_[0])):\n",
    "        for i in range(len(arr_)):\n",
    "            if(arr_[i][j] == 1):\n",
    "                ones_ = ones_+1\n",
    "            else:\n",
    "                zeros_ = zeros_+1\n",
    "        if(ones_ > zeros_):\n",
    "            pred_.append(1)\n",
    "        else:\n",
    "            pred_.append(0)\n",
    "        ones_ = 0\n",
    "        zeros_ = 0\n",
    "    ensemble_acc = accuracy_score(y_true = y,  y_pred = pred_)\n",
    "    print(\"Watermarked Model Accuracy: {:.3f}\".format(ensemble_acc))\n",
    "\n",
    "def compute_accuracies_standard_model(label, list_values, X_test, y_test, ensemble):\n",
    "    y_ = []\n",
    "    for i in range(len(list_values)):\n",
    "        y_.append(label) #label\n",
    "        \n",
    "    trigger_synt_acc = accuracy_score(y_true = y_, y_pred = rf_standard.predict(list_values))\n",
    "    print(\"Trigger synth acc: \", trigger_synt_acc, end = \"  -  \")\n",
    "    compute_accuracies_watermarked_model(ensemble, list_values, y_)\n",
    "    print(\"Test set: \", accuracy_score(y_true = y_test, y_pred = rf_standard.predict(X_test)), end = \"  -  \")\n",
    "    compute_accuracies_watermarked_model(ensemble, X_test, y_test)\n",
    "\n",
    "    return trigger_synt_acc\n",
    "\n",
    "def ensemble_SC(n_trees, watermark, ones, zeros, X_train, y_train, X_test, y_test, dim, max_depth, max_leaves, stats, standard_accuracy):\n",
    "\n",
    "    perc = dim*len(X_train)\n",
    "    random.seed(RANDOM_STATE)\n",
    "    all_instances = []\n",
    "    for x in X_train:\n",
    "        all_instances.append(x)\n",
    "    sample = random.sample(all_instances, int(perc))\n",
    "\n",
    "    labels_switched = []\n",
    "    for i, x_i in enumerate(X_train):\n",
    "        labels_switched.append(y_train[i])\n",
    "        for j, instance in enumerate(sample):\n",
    "            if np.array_equal(x_i, instance):\n",
    "                if(y_train[i] == 1): labels_switched[i] = 0\n",
    "                else: labels_switched[i] = 1\n",
    "\n",
    "    labels = []\n",
    "    for instance in sample:\n",
    "        for i, x_i in enumerate(X_train):\n",
    "            if np.array_equal(x_i, instance):\n",
    "                labels.append(y_train[i])\n",
    "\n",
    "    labels_s = copy.deepcopy(labels)\n",
    "    for i in range(len(labels_s)):\n",
    "        if labels_s[i] == 1:\n",
    "            labels_s[i] = 0\n",
    "        else:\n",
    "          labels_s[i] = 1\n",
    "\n",
    "    pos = []\n",
    "    for i, x_i in enumerate(X_train):\n",
    "            for s in sample:\n",
    "                if np.array_equal(x_i, s):\n",
    "                    pos.append(i)\n",
    "\n",
    "    peso = 2\n",
    "    cond = 0\n",
    "    weights = []\n",
    "    for i, x_i in enumerate(X_train):\n",
    "            weights.append(1)\n",
    "\n",
    "    n_attempt = 0\n",
    "    while cond != 1 and n_attempt < 500: \n",
    "        new_arr = [] \n",
    "        for i in pos:\n",
    "            weights[i] = peso\n",
    "\n",
    "        trigger_rf = RandomForestClassifier(n_estimators=ones, max_depth = max_depth, max_leaf_nodes =  max_leaves, random_state = RANDOM_STATE, bootstrap = False, n_jobs = 3)\n",
    "        trigger_rf.fit(X_train, labels_switched, sample_weight = weights)\n",
    "\n",
    "        tot = 0\n",
    "        num = 0\n",
    "        for i, s in enumerate(sample):\n",
    "            for t in trigger_rf.estimators_:\n",
    "                if t.predict((np.array([s,]))) != labels[i]:\n",
    "                    num = num+1\n",
    "            if num == len(trigger_rf.estimators_):\n",
    "              tot = tot + 1\n",
    "            num = 0\n",
    "\n",
    "        print(\"Percentage of trigger set instances correctly misclassified: {:.3f}\\n\\n\".format(tot/len(sample)))\n",
    "\n",
    "        if (cond < tot/len(sample)):\n",
    "            cond = tot/len(sample)\n",
    "            trigger_estimators = []\n",
    "            for t in trigger_rf.estimators_:\n",
    "                trigger_estimators.append(t)\n",
    "        peso = peso + 30\n",
    "        n_attempt += 1\n",
    "    \n",
    "    peso = 2\n",
    "    cond = 0\n",
    "    weights = []\n",
    "    for i, x_i in enumerate(X_train):\n",
    "            weights.append(1)\n",
    "    n_attempt = 0\n",
    "    while cond != 1 and n_attempt < 500: \n",
    "        new_arr = [] \n",
    "        for i in pos:\n",
    "            weights[i] = peso\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=zeros, max_depth = max_depth, n_jobs = 3, max_leaf_nodes = max_leaves, random_state = RANDOM_STATE, bootstrap = False)\n",
    "        rf.fit(X_train, y_train, sample_weight = weights)\n",
    "        #trigger_estimators = trigger_rf.estimators_\n",
    "\n",
    "        tot = 0\n",
    "        num = 0\n",
    "        for i, s in enumerate(sample):\n",
    "            for t in rf.estimators_:\n",
    "                if t.predict((np.array([s,]))) == labels[i]:\n",
    "                    num = num+1\n",
    "            if num == len(rf.estimators_):\n",
    "              tot = tot + 1\n",
    "            num = 0\n",
    "\n",
    "        if (cond < tot/len(sample)):\n",
    "            cond = tot/len(sample)\n",
    "            estimators = []\n",
    "            for t in rf.estimators_:\n",
    "                estimators.append(t)\n",
    "        peso = peso + 30\n",
    "        n_attempt += 1\n",
    "\n",
    "    tot = 0\n",
    "    for i, s in enumerate(sample):\n",
    "        for t in  rf.estimators_:\n",
    "            if t.predict((np.array([s,]))) == labels[i]:\n",
    "                num = num+1\n",
    "        if num == len( rf.estimators_):\n",
    "            tot = tot + 1\n",
    "        num = 0\n",
    "\n",
    "    print(\"Percentage of trigger set instances correctly classified: {:.3f}\\n\\n\".format(tot/len(sample)))\n",
    "\n",
    "    ensemble = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    for digit in watermark:\n",
    "        if digit == 1:\n",
    "            ensemble.append(trigger_estimators[j])\n",
    "            j = j+1\n",
    "        else:\n",
    "            ensemble.append(estimators[i])\n",
    "            i = i+1\n",
    "            \n",
    "    arr = []\n",
    "    for i, t in enumerate(ensemble):\n",
    "        arr.append(t.predict(X_test))\n",
    "        print(\"Label: {}  -  \".format(watermark[i]))\n",
    "        print(\"Number of leaves: \", end=\"\")\n",
    "        print(t.get_n_leaves(), end = \"  -  \")\n",
    "        print(\"Depth: \", end=\"\")\n",
    "        print(t.get_depth())\n",
    "\n",
    "    ones = 0\n",
    "    zeros = 0\n",
    "    pred = []\n",
    "    for j in range(len(arr[0])):\n",
    "        for i in range(len(arr)):\n",
    "            if(arr[i][j] == 1):\n",
    "                ones = ones+1\n",
    "            else:\n",
    "                zeros = zeros+1\n",
    "        if(ones > zeros):\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "        ones = 0\n",
    "        zeros = 0\n",
    "    ensemble_acc = accuracy_score(y_true = y_test,  y_pred = pred)\n",
    "    print(\"Ensemble Accuracy: {:.3f}\".format(ensemble_acc))\n",
    "    stats.append(ensemble_acc)\n",
    "    print(\"Standard Model Accuracy: {:.3f}\".format(standard_accuracy))\n",
    "    print()\n",
    "    return ensemble, sample, labels\n",
    "\n",
    "def extract_paths_from_tree_aux(node_id, n_nodes, current_path, children_left, children_right, features, thresholds, values):\n",
    "    if node_id >= n_nodes:\n",
    "        return\n",
    "\n",
    "    is_split_node = children_left[node_id] != children_right[node_id]\n",
    "    paths = []\n",
    "\n",
    "    if is_split_node:\n",
    "        paths = extract_paths_from_tree_aux(children_left[node_id], n_nodes, current_path + [(features[node_id], \"<=\", thresholds[node_id])], children_left, children_right, features, thresholds, values)\n",
    "        paths += extract_paths_from_tree_aux(children_right[node_id], n_nodes, current_path + [(features[node_id], \">\", thresholds[node_id])], children_left, children_right, features, thresholds, values)\n",
    "        return paths\n",
    "    else:\n",
    "        current_path.append((\"-1\", np.argmax(values[node_id])))\n",
    "        return [current_path]\n",
    "\n",
    "def extract_paths_from_tree(tree):\n",
    "\n",
    "    n_nodes = tree.tree_.node_count\n",
    "    children_left = tree.tree_.children_left\n",
    "    children_right = tree.tree_.children_right\n",
    "    features = tree.tree_.feature\n",
    "    thresholds = tree.tree_.threshold\n",
    "    values = tree.tree_.value\n",
    "\n",
    "    return extract_paths_from_tree_aux(0, n_nodes, [], children_left, children_right, features, thresholds, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyOGbnxSjNt9",
    "outputId": "5c3081b5-43bd-4972-fbdb-799b56fcdde0"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#load dataset\n",
    "X,y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X.to_numpy().astype(float)\n",
    "y = y.to_numpy().astype(int)\n",
    "\n",
    "#filter instances x labels\n",
    "X = X[np.isin(y, [2, 6])]\n",
    "y = y[np.isin(y, [2, 6])]\n",
    "y[y==2] = 0\n",
    "y[y==6] = 1\n",
    "\n",
    "\n",
    "#Scale in [0-1]\n",
    "#y = y.reshape((y.shape[0], 1))\n",
    "X = np.nan_to_num(X)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "#Save splittings\n",
    "\"\"\"dataset = np.concatenate((y, X), axis=1)\n",
    "dataset_df = pd.DataFrame(dataset)\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero Alberi:90\n",
      "Depth: 24\n"
     ]
    }
   ],
   "source": [
    "rf_standard = load(\"../data/best_forest_mnist.joblib\")\n",
    "n_trees = 0\n",
    "max_depth = 0\n",
    "dictionare  = rf_standard.get_params()\n",
    "for i, param in enumerate(dictionare):\n",
    "    if param == 'n_estimators':\n",
    "        n_trees = dictionare['n_estimators']\n",
    "    if param == 'max_depth':\n",
    "        max_depth =  dictionare['max_depth']\n",
    "print(\"Number of trees:{}\".format(n_trees))\n",
    "print(\"Depth: {}\".format(max_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n"
     ]
    }
   ],
   "source": [
    "array_leaves = []\n",
    "array_depth = []\n",
    "max_leaves = 0\n",
    "min_leaves = 100000\n",
    "for t in rf_standard.estimators_:\n",
    "    leaves = t.get_n_leaves()\n",
    "    array_leaves.append(leaves)\n",
    "    if max_leaves < leaves: max_leaves = leaves\n",
    "    if min_leaves > leaves: min_leaves = leaves\n",
    "\n",
    "    array_depth.append(t.get_depth())\n",
    "\n",
    "max_leaves = int(np.mean(array_leaves) - np.std(array_leaves))\n",
    "print(max_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "H-x5lErxMK3H",
    "outputId": "cc700712-f405-41bd-f4f4-943b6d49d860",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "watermark = []\n",
    "for i in range(int(0.5*n_trees)):\n",
    "    watermark.append(1)\n",
    "for i in range(n_trees - int(0.5*n_trees)):\n",
    "    watermark.append(0)\n",
    "random.shuffle(watermark)\n",
    "\n",
    "watermark = np.array(watermark)\n",
    "size = [0.01, 0.02, 0.03, 0.04]\n",
    "\n",
    "stats = []\n",
    "for dim in size:\n",
    "    RANDOM_STATE = RANDOM_STATE\n",
    "    ones = (watermark == 1).sum()\n",
    "    zeros = (watermark == 0).sum()\n",
    "    print(\"Watermark: \", watermark)\n",
    "    print(\"Size: {}\".format(dim))\n",
    "\n",
    "    rf_standard = load(\"../data/best_forest_mnist.joblib\")\n",
    "    standard_accuracy = accuracy_score(y_true = y_test, y_pred = rf_standard.predict(X_test))\n",
    "    base_stats = []\n",
    "    for i in range(len(size)):\n",
    "        base_stats.append(standard_accuracy)\n",
    "\n",
    "    ensemble_rit = ensemble_SC(n_trees, watermark, ones, zeros, X_train, y_train, X_test, y_test, dim, max_depth, max_leaves, stats, standard_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_x_trigger_dim = pd.DataFrame([size, stats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_standard_x_trigger_dim = pd.DataFrame([size, base_stats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"../results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_x_trigger_dim.to_csv(prefix + \"report_acc_x_trigger_dim_mnist.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_standard_x_trigger_dim.to_csv(prefix + \"report_acc_standard_x_trigger_dim_mnist.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perc = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "dim = 0.02\n",
    "stats = []\n",
    "for p in perc:\n",
    "    watermark = []\n",
    "    for i in range(int(p*n_trees)):\n",
    "        watermark.append(1)\n",
    "    for i in range(n_trees - int(p*n_trees)):\n",
    "        watermark.append(0)\n",
    "    random.shuffle(watermark)\n",
    "\n",
    "    watermark = np.array(watermark)\n",
    "    rf_standard = load(\"../data/best_forest_mnist.joblib\")\n",
    "    standard_accuracy = accuracy_score(y_true = y_test, y_pred = rf_standard.predict(X_test))\n",
    "    \n",
    "    ones = (watermark == 1).sum()\n",
    "    zeros = (watermark == 0).sum()\n",
    "    print(\"Watermark: \", watermark)\n",
    "    print(\"Size: {}\".format(dim))\n",
    "\n",
    "    ensemble_rit = ensemble_SC(n_trees, watermark, ones, zeros, X_train, y_train, X_test, y_test, dim, max_depth, max_leaves, stats, standard_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_stats = [standard_accuracy]*len(perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9975961538461539,\n",
       " 0.9975961538461539,\n",
       " 0.9975961538461539,\n",
       " 0.9975961538461539,\n",
       " 0.9975961538461539,\n",
       " 0.9975961538461539]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_x_perc_bit_1 = pd.DataFrame([perc, stats])\n",
    "acc_standard_x_perc_bit_1 = pd.DataFrame([perc, base_stats])\n",
    "acc_x_perc_bit_1.to_csv(prefix + \"report_acc_x_perc_bit_1_mnist.csv\", header=False, index=False)\n",
    "acc_standard_x_perc_bit_1.to_csv(prefix + \"report_acc_standard_x_perc_bit_1_mnist.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
