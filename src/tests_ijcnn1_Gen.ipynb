{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0xOMecfjsf4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from joblib import dump, load\n",
    "\n",
    "RANDOM_STATE = 7\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_forest(ensemble, filename):\n",
    "    dump(ensemble, filename  + \".joblib\")\n",
    "    \n",
    "def generate_random_bitmask(n_bits, perc_1):\n",
    "    attacker_bitmask = []\n",
    "    for i in range(int(perc_1*n_bits)):\n",
    "        attacker_bitmask.append(1)\n",
    "    for i in range(n_trees - int(perc_1*n_bits)):\n",
    "        attacker_bitmask.append(0)\n",
    "    random.shuffle(attacker_bitmask)\n",
    "    return attacker_bitmask\n",
    "    \n",
    "def extract_paths_from_bitmask(paths, attacker_bitmask, true_label):\n",
    "    paths_to_convert = []\n",
    "    for i, path_x_tree_list in enumerate(paths):\n",
    "        path_to_covert_x_tree = []\n",
    "        for path_in_tree in path_x_tree_list:\n",
    "            if path_in_tree[-1][1] == true_label and attacker_bitmask[i] == 0:\n",
    "                path_to_covert_x_tree.append(path_in_tree)\n",
    "            if path_in_tree[-1][1] != true_label and attacker_bitmask[i] == 1:\n",
    "                path_to_covert_x_tree.append(path_in_tree)\n",
    "        paths_to_convert.append(path_to_covert_x_tree)\n",
    "    return paths_to_convert\n",
    "\n",
    "def create_constraint_problem(paths_to_convert):\n",
    "    trees_paths_to_conj = []\n",
    "    list_elem = []\n",
    "    for paths_x_tree in paths_to_convert:\n",
    "        tree_paths_to_disj = []\n",
    "        for path_in_tree in paths_x_tree:\n",
    "            path = []\n",
    "            for node in path_in_tree[:-1]:\n",
    "                list_elem.append(node[0])\n",
    "                if(node[1] == '<='):\n",
    "                    constr = elem[node[0]] <= node[2] - 0.0001 \n",
    "                else:\n",
    "                    constr = elem[node[0]] > node[2] + 0.0001\n",
    "                path.append(constr)\n",
    "            tree_paths_to_disj.append(And(*path))\n",
    "        trees_paths_to_conj.append(Or(*tree_paths_to_disj))\n",
    "    constr = And(*trees_paths_to_conj)\n",
    "    return constr, list_elem\n",
    "\n",
    "def generate_instances(constraints, n_instances, list_elem, ensemble):\n",
    "    list_status_problems = []\n",
    "    list_values = []\n",
    "    tot_time = 0\n",
    "    for i in range(n_instances):\n",
    "        s = Solver()\n",
    "        s.add(constraints)\n",
    "        res = s.check()\n",
    "        list_status_problems.append(res)\n",
    "        if res == z3.sat:\n",
    "            #print(\"Sat {}\".format(i), end = \"  -  \")\n",
    "            for k, v in s.statistics():\n",
    "                if(k == 'time'):\n",
    "                    #print(\"%s : %s\" % (k, v))\n",
    "                    tot_time += v\n",
    "            m = s.model()\n",
    "            list_internal = []\n",
    "            sol_values_constr = []\n",
    "            list_elem = list(set(list_elem))\n",
    "            found_instance_from_z3 = []\n",
    "            for i in range(len(list_elem)):\n",
    "                sol_values_constr.append(elem[i] == m[elem[i]])\n",
    "            #print(Not(And(*sol_values_constr)))\n",
    "            constraints = And(constraints, Not(And(*sol_values_constr))) \n",
    "            for i in range(ensemble.n_features_in_):\n",
    "                if m[elem[i]] == None:\n",
    "                    list_internal.append(float(0))\n",
    "                else:\n",
    "                    frac = m[elem[i]].as_fraction()\n",
    "                    list_internal.append(float(frac.numerator) / float(frac.denominator))\n",
    "            list_values.append(list_internal)\n",
    "        else:\n",
    "            #print(res, end = \", \")\n",
    "            break\n",
    "        \n",
    "    return list_status_problems, list_values, tot_time, res\n",
    "\n",
    "def check_predictions_with_watermark(instances, labels, ensemble, watermark):\n",
    "    for instance, label in zip(instances, labels):\n",
    "        for i, tree in enumerate(ensemble):\n",
    "            predicted_label = int(tree.predict(np.array(instance).reshape((1, len(instance))))[0])\n",
    "            if (watermark[i] == 0 and predicted_label != label) or (watermark[i] == 1 and predicted_label == label):\n",
    "                print(\"Broken prediction on tree {} with class {} from tree, true label {} and watermark bit {}\".format(i, predicted_label, label, watermark[i]))\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def generate_extra_constraint(constr, eps, elem, test_instance):\n",
    "    constr_add = []\n",
    "    for i in range(len(elem)):\n",
    "            constr_add.append(elem[i] >= test_instance[i] - eps)\n",
    "            constr_add.append(elem[i] <= test_instance[i] + eps)\n",
    "            constr_add.append(elem[i] >= 0)\n",
    "            constr_add.append(elem[i] <= 1)\n",
    "    constr = And(constr, And(*constr_add))\n",
    "    return constr\n",
    "\n",
    "def generate_range_constraint(constr, eps, elem, test_instance):\n",
    "    constr_add = []\n",
    "    for i in range(len(elem)):\n",
    "            constr_add.append(elem[i] >= 0)\n",
    "            constr_add.append(elem[i] <= 1)\n",
    "    constr = And(constr, And(*constr_add))\n",
    "    return constr\n",
    "\n",
    "def generate_random_instance():\n",
    "    synt_instance = []\n",
    "    for f in range(len(elem)):\n",
    "        synt_instance.append(random.random()) \n",
    "    #random label\n",
    "    synt_instance_label = random.randint(0,1)\n",
    "    return synt_instance, synt_instance_label\n",
    "\n",
    "def generate_bitmask(instance, instance_label): \n",
    "    attacker_bitmask = []\n",
    "    for t in ensemble_rit:\n",
    "        for pred in t.predict([instance]):\n",
    "            if pred == instance_label:\n",
    "                attacker_bitmask.append(0)\n",
    "            else:\n",
    "                attacker_bitmask.append(1)\n",
    "    return attacker_bitmask\n",
    "\n",
    "def compute_accuracies_watermarked_model(ensemble, X, y):\n",
    "    arr_ = []\n",
    "    for i, tree in enumerate(ensemble):\n",
    "        arr_.append(tree.predict(X))\n",
    "        \n",
    "    ones_ = 0\n",
    "    zeros_ = 0\n",
    "    pred_ = []\n",
    "    for j in range(len(arr_[0])):\n",
    "        for i in range(len(arr_)):\n",
    "            if(arr_[i][j] == 1):\n",
    "                ones_ = ones_+1\n",
    "            else:\n",
    "                zeros_ = zeros_+1\n",
    "        if(ones_ > zeros_):\n",
    "            pred_.append(1)\n",
    "        else:\n",
    "            pred_.append(0)\n",
    "        ones_ = 0\n",
    "        zeros_ = 0\n",
    "    ensemble_acc = accuracy_score(y_true = y,  y_pred = pred_)\n",
    "    print(\"Watermarked Model Accuracy: {:.3f}\".format(ensemble_acc))\n",
    "\n",
    "def compute_accuracies_standard_model(label, list_values, X_test, y_test, ensemble):\n",
    "    y_ = []\n",
    "    for i in range(len(list_values)):\n",
    "        y_.append(label) #label\n",
    "        \n",
    "    trigger_synt_acc = accuracy_score(y_true = y_, y_pred = rf_standard.predict(list_values))\n",
    "    print(\"Trigger synth acc: \", trigger_synt_acc, end = \"  -  \")\n",
    "    compute_accuracies_watermarked_model(ensemble, list_values, y_)\n",
    "    print(\"Test set: \", accuracy_score(y_true = y_test, y_pred = rf_standard.predict(X_test)), end = \"  -  \")\n",
    "    compute_accuracies_watermarked_model(ensemble, X_test, y_test)\n",
    "\n",
    "    return trigger_synt_acc\n",
    "\n",
    "def ensemble_SC(n_trees, watermark, ones, zeros, X_train, y_train, X_test, y_test, dim, max_depth, max_leaves, stats, standard_accuracy):\n",
    "\n",
    "    perc = dim*len(X_train)\n",
    "    random.seed(RANDOM_STATE)\n",
    "    all_instances = []\n",
    "    for x in X_train:\n",
    "        all_instances.append(x)\n",
    "    sample = random.sample(all_instances, int(perc))\n",
    "\n",
    "    labels_switched = []\n",
    "    for i, x_i in enumerate(X_train):\n",
    "        labels_switched.append(y_train[i])\n",
    "        for j, instance in enumerate(sample):\n",
    "            if np.array_equal(x_i, instance):\n",
    "                if(y_train[i] == 1): labels_switched[i] = 0\n",
    "                else: labels_switched[i] = 1\n",
    "\n",
    "    labels = []\n",
    "    for instance in sample:\n",
    "        for i, x_i in enumerate(X_train):\n",
    "            if np.array_equal(x_i, instance):\n",
    "                labels.append(y_train[i])\n",
    "\n",
    "    labels_s = copy.deepcopy(labels)\n",
    "    for i in range(len(labels_s)):\n",
    "        if labels_s[i] == 1:\n",
    "            labels_s[i] = 0\n",
    "        else:\n",
    "          labels_s[i] = 1\n",
    "\n",
    "    pos = []\n",
    "    for i, x_i in enumerate(X_train):\n",
    "            for s in sample:\n",
    "                if np.array_equal(x_i, s):\n",
    "                    pos.append(i)\n",
    "\n",
    "    peso = 2\n",
    "    cond = 0\n",
    "    weights = []\n",
    "    for i, x_i in enumerate(X_train):\n",
    "            weights.append(1)\n",
    "\n",
    "    n_attempt = 0\n",
    "    while cond != 1 and n_attempt < 500: \n",
    "        new_arr = [] \n",
    "        for i in pos:\n",
    "            weights[i] = peso\n",
    "\n",
    "        trigger_rf = RandomForestClassifier(n_estimators=ones, max_depth = max_depth, max_leaf_nodes =  max_leaves, random_state = RANDOM_STATE, bootstrap = False, n_jobs = 3)\n",
    "        trigger_rf.fit(X_train, labels_switched, sample_weight = weights)\n",
    "\n",
    "        tot = 0\n",
    "        num = 0\n",
    "        for i, s in enumerate(sample):\n",
    "            for t in trigger_rf.estimators_:\n",
    "                if t.predict((np.array([s,]))) != labels[i]:\n",
    "                    num = num+1\n",
    "            if num == len(trigger_rf.estimators_):\n",
    "              tot = tot + 1\n",
    "            num = 0\n",
    "\n",
    "        print(\"Percentage of trigger set instances correctly misclassified: {:.3f}\\n\\n\".format(tot/len(sample)))\n",
    "\n",
    "        if (cond < tot/len(sample)):\n",
    "            cond = tot/len(sample)\n",
    "            trigger_estimators = []\n",
    "            for t in trigger_rf.estimators_:\n",
    "                trigger_estimators.append(t)\n",
    "        peso = peso + 30\n",
    "        n_attempt += 1\n",
    "    \n",
    "    peso = 2\n",
    "    cond = 0\n",
    "    weights = []\n",
    "    for i, x_i in enumerate(X_train):\n",
    "            weights.append(1)\n",
    "    n_attempt = 0\n",
    "    while cond != 1 and n_attempt < 500: \n",
    "        new_arr = [] \n",
    "        for i in pos:\n",
    "            weights[i] = peso\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=zeros, max_depth = max_depth, n_jobs = 3, max_leaf_nodes = max_leaves, random_state = RANDOM_STATE, bootstrap = False)\n",
    "        rf.fit(X_train, y_train, sample_weight = weights)\n",
    "        #trigger_estimators = trigger_rf.estimators_\n",
    "\n",
    "        tot = 0\n",
    "        num = 0\n",
    "        for i, s in enumerate(sample):\n",
    "            for t in rf.estimators_:\n",
    "                if t.predict((np.array([s,]))) == labels[i]:\n",
    "                    num = num+1\n",
    "            if num == len(rf.estimators_):\n",
    "              tot = tot + 1\n",
    "            num = 0\n",
    "\n",
    "        if (cond < tot/len(sample)):\n",
    "            cond = tot/len(sample)\n",
    "            estimators = []\n",
    "            for t in rf.estimators_:\n",
    "                estimators.append(t)\n",
    "        peso = peso + 30\n",
    "        n_attempt += 1\n",
    "\n",
    "    tot = 0\n",
    "    for i, s in enumerate(sample):\n",
    "        for t in  rf.estimators_:\n",
    "            if t.predict((np.array([s,]))) == labels[i]:\n",
    "                num = num+1\n",
    "        if num == len( rf.estimators_):\n",
    "            tot = tot + 1\n",
    "        num = 0\n",
    "\n",
    "    print(\"Percentage of trigger set instances correctly classified: {:.3f}\\n\\n\".format(tot/len(sample)))\n",
    "\n",
    "    ensemble = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    for digit in watermark:\n",
    "        if digit == 1:\n",
    "            ensemble.append(trigger_estimators[j])\n",
    "            j = j+1\n",
    "        else:\n",
    "            ensemble.append(estimators[i])\n",
    "            i = i+1\n",
    "            \n",
    "    arr = []\n",
    "    for i, t in enumerate(ensemble):\n",
    "        arr.append(t.predict(X_test))\n",
    "        print(\"Label: {}  -  \".format(watermark[i]))\n",
    "        print(\"Number of leaves: \", end=\"\")\n",
    "        print(t.get_n_leaves(), end = \"  -  \")\n",
    "        print(\"Depth: \", end=\"\")\n",
    "        print(t.get_depth())\n",
    "\n",
    "    ones = 0\n",
    "    zeros = 0\n",
    "    pred = []\n",
    "    for j in range(len(arr[0])):\n",
    "        for i in range(len(arr)):\n",
    "            if(arr[i][j] == 1):\n",
    "                ones = ones+1\n",
    "            else:\n",
    "                zeros = zeros+1\n",
    "        if(ones > zeros):\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "        ones = 0\n",
    "        zeros = 0\n",
    "    ensemble_acc = accuracy_score(y_true = y_test,  y_pred = pred)\n",
    "    print(\"Ensemble Accuracy: {:.3f}\".format(ensemble_acc))\n",
    "    stats.append(ensemble_acc)\n",
    "    print(\"Standard Model Accuracy: {:.3f}\".format(standard_accuracy))\n",
    "    print()\n",
    "    return ensemble, sample, labels\n",
    "\n",
    "def extract_paths_from_tree_aux(node_id, n_nodes, current_path, children_left, children_right, features, thresholds, values):\n",
    "    if node_id >= n_nodes:\n",
    "        return\n",
    "\n",
    "    is_split_node = children_left[node_id] != children_right[node_id]\n",
    "    paths = []\n",
    "\n",
    "    if is_split_node:\n",
    "        paths = extract_paths_from_tree_aux(children_left[node_id], n_nodes, current_path + [(features[node_id], \"<=\", thresholds[node_id])], children_left, children_right, features, thresholds, values)\n",
    "        paths += extract_paths_from_tree_aux(children_right[node_id], n_nodes, current_path + [(features[node_id], \">\", thresholds[node_id])], children_left, children_right, features, thresholds, values)\n",
    "        return paths\n",
    "    else:\n",
    "        current_path.append((\"-1\", np.argmax(values[node_id])))\n",
    "        return [current_path]\n",
    "\n",
    "def extract_paths_from_tree(tree):\n",
    "\n",
    "    n_nodes = tree.tree_.node_count\n",
    "    children_left = tree.tree_.children_left\n",
    "    children_right = tree.tree_.children_right\n",
    "    features = tree.tree_.feature\n",
    "    thresholds = tree.tree_.threshold\n",
    "    values = tree.tree_.value\n",
    "\n",
    "    return extract_paths_from_tree_aux(0, n_nodes, [], children_left, children_right, features, thresholds, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyOGbnxSjNt9",
    "outputId": "5c3081b5-43bd-4972-fbdb-799b56fcdde0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "f = open(\"../data/ijcnn1.tr\", \"r\")\n",
    "lines = f.readlines()\n",
    "n_features = 22\n",
    "data = np.zeros((len(lines), n_features+1))\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "\tsplitted_line = line.split(\" \")\n",
    "\tdata[i][0] = splitted_line[0]\n",
    "\tfor values in splitted_line[1:]:\n",
    "\t\tv_splitted = values.split(\":\")\n",
    "\t\tif len(v_splitted) == 2:\n",
    "\t\t\tf, v = v_splitted\n",
    "\t\t\tdata[i][int(f)] = v\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Values not parsed correctly! Line {}\".format(i))\n",
    "\t\t\tprint(v_splitted)\n",
    "\n",
    "print(\"Saving data!\")\n",
    "np.savetxt(\"../data/icjnn1_train.csv\", data, delimiter=\",\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train set\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "data = pd.read_csv(\"../data/icjnn1_train.csv\", delimiter=\",\", header=None)\n",
    "data = resample(data, n_samples=10000, random_state=RANDOM_STATE, replace=False)\n",
    "y_train = data.iloc[:, 0] = data.iloc[:, 0].astype(int).replace(-1, 0)\n",
    "y_train = y_train.values\n",
    "X_train = data.iloc[:, 1:] = data.iloc[:, 1:].astype(float).fillna(0)\n",
    "X_train = X_train.values\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "\n",
    "#Test set\n",
    "data = pd.read_csv(\"../data/icjnn1_test.csv\", delimiter=\",\", header=None)\n",
    "data = resample(data, n_samples=10000, random_state=RANDOM_STATE, replace=False)\n",
    "y_test = data.iloc[:, 0] = data.iloc[:, 0].astype(int).replace(-1, 0)\n",
    "y_test = y_test.values\n",
    "X_test = data.iloc[:, 1:] = data.iloc[:, 1:].astype(float).fillna(0)\n",
    "X_test = X_test.values\n",
    "\n",
    "X_test = np.nan_to_num(X_test)\n",
    "X_test = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnimXQ5dKtSW"
   },
   "outputs": [],
   "source": [
    "rf_init = RandomForestClassifier(n_estimators = 90, random_state = RANDOM_STATE, bootstrap = False, n_jobs  = 3)\n",
    "rf_init.fit(X_train,y_train)\n",
    "estimators = rf_init.estimators_\n",
    "\n",
    "max_leaves, max_depth, min_leaves, min_depth = 0, 0, 1000000, 1000000\n",
    "for t in estimators:\n",
    "    leaves = t.get_n_leaves()\n",
    "    depth = t.get_depth()\n",
    "    if max_leaves < leaves: max_leaves = leaves\n",
    "    if max_depth < depth: max_depth = depth\n",
    "    if min_leaves > leaves: min_leaves = leaves\n",
    "    if min_depth > depth: min_depth = depth\n",
    "\n",
    "print(\"Number of leaves: {} - {}\".format(min_leaves, max_leaves))\n",
    "print(\"Depth: {} - {}\".format(min_depth, max_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = list(range(50,101, 10))\n",
    "max_depth = list(range(4,27, 2))\n",
    "#max_leaf_nodes = list(range(80,101, 5))\n",
    "bootstrap = [False]\n",
    "random_state = [RANDOM_STATE]\n",
    "n_jobs = [6]\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "              'max_depth': max_depth,\n",
    "              #'max_leaf_nodes': max_leaf_nodes,\n",
    "              'bootstrap': bootstrap,\n",
    "              'random_state': random_state,\n",
    "              'n_jobs': n_jobs}\n",
    "\n",
    "rf_grid = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator = rf_grid, param_grid = param_grid, cv = 3, n_jobs = 6, verbose=4)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_grid = grid_search.best_estimator_\n",
    "\n",
    "n_trees = 0\n",
    "max_depth = 0\n",
    "dictionare  = best_grid.get_params()\n",
    "for i, param in enumerate(dictionare):\n",
    "    if param == 'n_estimators':\n",
    "        n_trees = dictionare['n_estimators']\n",
    "    if param == 'max_depth':\n",
    "        max_depth =  dictionare['max_depth']\n",
    "print(\"Number of trees:{}\".format(n_trees))\n",
    "print(\"Depth: {}\".format(max_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_forest(best_grid, \"../data/best_forest_ijcnn12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_leaves = []\n",
    "array_depth = []\n",
    "max_leaves = 0\n",
    "min_leaves = 100000\n",
    "for t in best_grid.estimators_:\n",
    "    leaves = t.get_n_leaves()\n",
    "    array_leaves.append(leaves)\n",
    "    if max_leaves < leaves: max_leaves = leaves\n",
    "    if min_leaves > leaves: min_leaves = leaves\n",
    "\n",
    "    array_depth.append(t.get_depth())\n",
    "\n",
    "max_leaves = int(np.mean(array_leaves) - np.std(array_leaves))\n",
    "print(max_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(array_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "watermark = []\n",
    "for i in range(int(0.5*n_trees)):\n",
    "    watermark.append(1)\n",
    "for i in range(n_trees - int(0.5*n_trees)):\n",
    "    watermark.append(0)\n",
    "random.shuffle(watermark)\n",
    "\n",
    "watermark = np.array(watermark)\n",
    "dim = 0.02\n",
    "\n",
    "leaves_per_forest = [] \n",
    "ones = (watermark == 1).sum()\n",
    "zeros = (watermark == 0).sum()\n",
    "print(\"Watermark: \", watermark)\n",
    "print(\"Size: {}\".format(dim))\n",
    "stats = []\n",
    "standard_accuracy = accuracy_score(y_true = y_test, y_pred = best_grid.predict(X_test))\n",
    "\n",
    "ensemble_rit = ensemble_SC(n_trees, watermark, ones, zeros, X_train, y_train, X_test, y_test, dim, max_depth, max_leaves, stats, standard_accuracy)\n",
    "save_forest(ensemble_rit,\"../data/best_wm_forest_ijcnn12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(watermark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute standard deviation and means\n",
    "num_leaves_bit_0 = []\n",
    "num_leaves_bit_1 = []\n",
    "depths_bit_0 = []\n",
    "depths_bit_1 = []\n",
    "for i, tree in enumerate(ensemble_rit):\n",
    "    if watermark[i] == 1: \n",
    "        num_leaves_bit_1.append(tree.get_n_leaves())\n",
    "        depths_bit_1.append(tree.get_depth())\n",
    "    if watermark[i] == 0:\n",
    "        num_leaves_bit_0.append(tree.get_n_leaves())\n",
    "        depths_bit_0.append(tree.get_depth())\n",
    "\n",
    "std_leaves_bit_1 = np.std(num_leaves_bit_1)\n",
    "means_leaves_bit_1 = np.mean(num_leaves_bit_1)\n",
    "std_leaves_bit_0 = np.std(num_leaves_bit_0)\n",
    "means_leaves_bit_0 = np.mean(num_leaves_bit_0)\n",
    "print(\"LEAVES\")\n",
    "print(\"Bit 1: Std {} - Mean {}\".format(std_leaves_bit_1, means_leaves_bit_1))\n",
    "print(\"Bit 0: Std {} - Mean {}\".format(std_leaves_bit_0, means_leaves_bit_0))\n",
    "print()\n",
    "\n",
    "std_depth_bit_1 = np.std(depths_bit_1)\n",
    "means_depth_bit_1 = np.mean(depths_bit_1)\n",
    "std_depth_bit_0 = np.std(depths_bit_0)\n",
    "means_depth_bit_0 = np.mean(depths_bit_0)\n",
    "print(\"DEPTH\")\n",
    "print(\"Bit 1: Std {} - Mean {}\".format(std_depth_bit_1, means_depth_bit_1))\n",
    "print(\"Bit 0: Std {} - Mean {}\".format(std_depth_bit_0, means_depth_bit_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = len(depths_bit_0 + depths_bit_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_depths = np.mean(depths_bit_0 + depths_bit_1)\n",
    "std_depths = np.std(depths_bit_0 + depths_bit_1)\n",
    "std_criteria_depths = std_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_1_depths = sum(depths_bit_0 > mean_depths + std_criteria_depths)\n",
    "correct_0_depths = sum(depths_bit_0 < mean_depths - std_criteria_depths)\n",
    "correct_1_depths = sum(depths_bit_1 > mean_depths + std_criteria_depths)\n",
    "wrong_0_depths = sum(depths_bit_1 < mean_depths - std_criteria_depths)\n",
    "uncertain = n_trees - wrong_1_depths - wrong_0_depths - correct_0_depths - correct_1_depths\n",
    "\n",
    "print(\"BIT 1 WRONG: \", wrong_1_depths)\n",
    "print(\"BIT 0 CORRECT: \", correct_0_depths)\n",
    "print(\"BIT 1 CORRECT: \", correct_1_depths)\n",
    "print(\"BIT 0 WRONG: \", wrong_0_depths)\n",
    "print(\"UNCERTAIN: \", uncertain)\n",
    "print(\"CORRECT: \", correct_0_depths + correct_1_depths)\n",
    "print(\"WRONG: \", wrong_0_depths + wrong_1_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_1_depths = sum(depths_bit_0 > mean_depths)\n",
    "correct_0_depths = sum(depths_bit_0 < mean_depths)\n",
    "correct_1_depths = sum(depths_bit_1 > mean_depths)\n",
    "wrong_0_depths = sum(depths_bit_1 < mean_depths)\n",
    "uncertain = n_trees - wrong_1_depths - wrong_0_depths - correct_0_depths - correct_1_depths\n",
    "\n",
    "print(\"BIT 1 WRONG: \", wrong_1_depths)\n",
    "print(\"BIT 0 CORRECT: \", correct_0_depths)\n",
    "print(\"BIT 1 CORRECT: \", correct_1_depths)\n",
    "print(\"BIT 0 WRONG: \", wrong_0_depths)\n",
    "print(\"UNCERTAIN: \", uncertain)\n",
    "print(\"CORRECT: \", correct_0_depths + correct_1_depths)\n",
    "print(\"WRONG: \", wrong_0_depths + wrong_1_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_num_leaves = np.mean(num_leaves_bit_0 + num_leaves_bit_1)\n",
    "std_num_leaves = np.std(num_leaves_bit_0 + num_leaves_bit_1)\n",
    "std_criteria_leaves = std_num_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DEPTH\")\n",
    "print(\"mean: \", mean_depths)\n",
    "print(\"std: \", std_depths)\n",
    "\n",
    "print(\"LEAVES\")\n",
    "print(\"mean: \", mean_num_leaves)\n",
    "print(\"std: \", std_num_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_1_leaves = sum(num_leaves_bit_0 > mean_num_leaves + std_criteria_leaves)\n",
    "correct_0_leaves = sum(num_leaves_bit_0 < mean_num_leaves - std_criteria_leaves)\n",
    "correct_1_leaves = sum(num_leaves_bit_1 > mean_num_leaves + std_criteria_leaves)\n",
    "wrong_0_leaves = sum(num_leaves_bit_1 < mean_num_leaves - std_criteria_leaves)\n",
    "uncertain = n_trees - wrong_1_leaves - wrong_0_leaves - correct_0_leaves - correct_1_leaves\n",
    "\n",
    "print(\"BIT 1 WRONG: \", wrong_1_leaves)\n",
    "print(\"BIT 0 CORRECT: \", correct_0_leaves)\n",
    "print(\"BIT 1 CORRECT: \", correct_1_leaves)\n",
    "print(\"BIT 0 WRONG: \", wrong_0_leaves)\n",
    "print(\"UNCERTAIN: \", uncertain)\n",
    "print(\"CORRECT: \", correct_0_leaves + correct_1_leaves)\n",
    "print(\"WRONG: \", wrong_0_leaves + wrong_1_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrong_1_leaves = sum(num_leaves_bit_0 > mean_num_leaves)\n",
    "correct_0_leaves = sum(num_leaves_bit_0 < mean_num_leaves)\n",
    "correct_1_leaves = sum(num_leaves_bit_1 > mean_num_leaves)\n",
    "wrong_0_leaves = sum(num_leaves_bit_1 < mean_num_leaves)\n",
    "uncertain = n_trees - wrong_1_leaves - wrong_0_leaves - correct_0_leaves - correct_1_leaves\n",
    "\n",
    "print(\"BIT 1 WRONG: \", wrong_1_leaves)\n",
    "print(\"BIT 0 CORRECT: \", correct_0_leaves)\n",
    "print(\"BIT 1 CORRECT: \", correct_1_leaves)\n",
    "print(\"BIT 0 WRONG: \", wrong_0_leaves)\n",
    "print(\"UNCERTAIN: \", uncertain)\n",
    "print(\"CORRECT: \", correct_0_leaves + correct_1_leaves)\n",
    "print(\"WRONG: \", wrong_0_leaves + wrong_1_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_leaves = []\n",
    "depths = []\n",
    "for i, tree in enumerate(ensemble_rit):\n",
    "    num_leaves.append(tree.get_n_leaves())\n",
    "    depths.append(tree.get_depth())\n",
    "\n",
    "data.to_csv(\"../results/stats_wm_model_ijcnn1.csv\")\n",
    "data = pd.DataFrame([watermark, depths, num_leaves], index=[\"Bits\", \"Depths\", \"Leaves\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3.3 new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoiYyc_Knb4i"
   },
   "outputs": [],
   "source": [
    "from z3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = load(\"../data/best_forest_ijcnn12.joblib\")\n",
    "ensemble_rit = load(\"../data/best_wm_forest_ijcnn12.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ensemble_rit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0XJU978ngDZ"
   },
   "outputs": [],
   "source": [
    "paths = []\n",
    "for i in range(len(ensemble_rit)):\n",
    "    paths.append(extract_paths_from_tree(ensemble_rit[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZPUSTlRnleE",
    "outputId": "b468d0b5-29a7-4430-88ec-b0064846c608",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "elem = []\n",
    "for i in range(len(X_train[0,:])):\n",
    "    elem.append(Real('x'+str(i)))\n",
    "\n",
    "#print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "n_trees = len(ensemble_rit)\n",
    "#int(0.2*len(X_train))\n",
    "subset_test_set_available_to_attacker, subset_labels_test_set_available_to_attacker = resample(X_test, y_test, n_samples=int(0.02*len(X_train)), random_state=RANDOM_STATE, replace=False)\n",
    "print(\"N instances trigger set: \", int(0.02*len(X_train)))\n",
    "trigger_sets_x_bitmask = []\n",
    "bitmasks = []\n",
    "results_x_bitmasks = []\n",
    "epsilon_list = [0.3]#list(np.arange(0.1, 1.1, 0.2))\n",
    "time_list = []\n",
    "\n",
    "for epsilon in epsilon_list:\n",
    "    print(\"RUN FOR EPSILON: \", epsilon)\n",
    "    trigger_set_x_bitmask_x_eps = []\n",
    "    results_x_bitmask_x_eps = []\n",
    "    bitmasks_x_eps = []\n",
    "    time_x_eps = []\n",
    "    random.seed(RANDOM_STATE)\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    for i in range(5):\n",
    "        print(\"RUN FOR BITMASK: \", i+1)\n",
    "        trigger_set = []\n",
    "        #take test set instance\n",
    "        #generate random bitmask\n",
    "        attacker_bitmask = generate_random_bitmask(n_trees, 0.5)\n",
    "        print(\"Attacker bitmask: \", attacker_bitmask)\n",
    "        #label 1\n",
    "        paths_to_convert = extract_paths_from_bitmask(paths, attacker_bitmask, 1)\n",
    "        constr, list_elem = create_constraint_problem(paths_to_convert)\n",
    "        #label 0\n",
    "        paths_to_convert_2 = extract_paths_from_bitmask(paths, attacker_bitmask, 0)\n",
    "        constr_2, list_elem_2 = create_constraint_problem(paths_to_convert_2)\n",
    "        \n",
    "        trigger_set_x_bitmask = []\n",
    "        results_x_bitmask = []\n",
    "        start = time.time()\n",
    "        for idx in range(len(subset_test_set_available_to_attacker)):\n",
    "            print(\"RUN FOR INSTANCE: \", idx)\n",
    "            synthetic_instance = subset_test_set_available_to_attacker[idx]\n",
    "            true_label = subset_labels_test_set_available_to_attacker[idx]\n",
    "    \n",
    "            if(true_label == 1):\n",
    "                updated_constr = generate_extra_constraint(constr, epsilon, elem, synthetic_instance)\n",
    "                list_status_problems, list_values, tot_time, res = generate_instances(updated_constr, 1, list_elem, best_rf)\n",
    "            if(true_label == 0):\n",
    "                updated_constr2 = generate_extra_constraint(constr_2, epsilon, elem, synthetic_instance)\n",
    "                list_status_problems, list_values, tot_time, res = generate_instances(updated_constr2, 1, list_elem_2, best_rf)\n",
    "        \n",
    "            results_x_bitmask.append(res)\n",
    "            if(res != unsat):\n",
    "                print(\"SAT\", end = \", \")\n",
    "                trigger_set_x_bitmask.append([true_label] + list_values[0])\n",
    "                print(\"Total time required to generate instances with true label {}: {}s\".format(true_label, tot_time))\n",
    "            \n",
    "                check_instances_0 = check_predictions_with_watermark(list_values, [true_label]*len(list_values), ensemble_rit, attacker_bitmask)\n",
    "                print(\"Check instances {}: {}\".format(true_label, check_instances_0))\n",
    "                if check_instances_0 == False:\n",
    "                    for i, val in enumerate(list_values[0]):\n",
    "                        print(\"{} -  {}\".format(i, val))\n",
    "                assert check_instances_0 == True\n",
    "                print()\n",
    "\n",
    "        end = time.time()\n",
    "        time_x_eps.append(end-start)\n",
    "        trigger_set_x_bitmask_x_eps.append(trigger_set_x_bitmask)\n",
    "        bitmasks_x_eps.append(attacker_bitmask)\n",
    "        results_x_bitmask_x_eps.append(results_x_bitmask)\n",
    "\n",
    "    time_list.append(time_x_eps)\n",
    "    results_x_bitmasks.append(results_x_bitmask_x_eps)\n",
    "    trigger_sets_x_bitmask.append(trigger_set_x_bitmask_x_eps)\n",
    "    bitmasks.append(bitmasks_x_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_x_bitmasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_x_configs = []\n",
    "for attempts_x_epsilon in results_x_bitmasks:\n",
    "    sat_x_epsilon = []\n",
    "    for attempts_x_bitmask in attempts_x_epsilon:\n",
    "        sat_x_epsilon.append(sum(np.array(attempts_x_bitmask) == z3.sat))\n",
    "    sat_x_configs.append(sat_x_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_x_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_x_epsilon_x_bitmask = pd.DataFrame(sat_x_configs, index=list(np.arange(0.1, 1.1, 0.2)))\n",
    "sat_x_epsilon_x_bitmask.to_csv(\"../results/report_sat_x_epsilon_x_bitmask_ijcnn1.csv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ei, epsilon in enumerate(list(np.arange(0.1, 1.1, 0.2))):\n",
    "    for bitmask_idx in range(5):\n",
    "        pd.DataFrame(trigger_sets_x_bitmask[ei][bitmask_idx]).to_csv(\"../results/synth_trigger_set_ijcnn1_{}_{}.csv\".format(str(epsilon)[:3], bitmask_idx), header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_x_epsilon_x_bitmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trigger_sets_x_bitmask[1][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "pixels = np.array(trigger_sets_x_bitmask[1][0][1][1:]).reshape((28, 28))\n",
    "ax[0].imshow(pixels, cmap='gray')\n",
    "\n",
    "pixels = np.array(trigger_sets_x_bitmask[4][0][1][1:]).reshape((28, 28))\n",
    "ax[1].imshow(pixels, cmap='gray')\n",
    "\n",
    "plt.savefig(\"../results/example_z3_instances_03_09.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
